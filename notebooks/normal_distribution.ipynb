{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#TODO: just make it a package\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from tiny_xgboost import TinyXGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def small_X_y_data(n_samples=5_000):\n",
    "    \"\"\"Small set of X, y data (single feature)\"\"\"\n",
    "\n",
    "    def true_function(X):\n",
    "        return np.sin(3 * X)\n",
    "\n",
    "    def true_noise_scale(X):\n",
    "        return np.abs(np.cos(X))\n",
    "\n",
    "    np.random.seed(1234)\n",
    "    X = np.random.uniform(-2, -1, n_samples)\n",
    "    y = true_function(X) + np.random.normal(scale=true_noise_scale(X), size=n_samples)\n",
    "\n",
    "    return X.reshape(-1,1), y\n",
    "\n",
    "\n",
    "def gary_function(n_samples=5_000):\n",
    "    \n",
    "    X = np.random.uniform(0,1, n_samples)\n",
    "    y_mean = 3*X +2 + 4*np.where(X>0.4,1,0)\n",
    "    noise_size= y_mean/10\n",
    "    \n",
    "    outcomes = y_mean + np.random.normal(0, noise_size, n_samples)\n",
    "    \n",
    "    return X[..., np.newaxis], outcomes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X, y = small_X_y_data(1_500)\n",
    "X, y = gary_function(10_000)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before Running Tiny XGB, let's run homoskedastic XGB to get a good idea for what the log-likelihood to beat is\n",
    "\n",
    "Loss is given by\n",
    "\n",
    "$$\\frac{\\sigma ^{2}}{2}\\left[\\sum_{i=1}^{N}(y_{i} - f(x_{i}))^{2}\\right] + N \\ln \\sigma$$\n",
    "\n",
    "\n",
    "Once we have the residuals, we can just call these $r_{i}$ so we have\n",
    "\n",
    "$$\\frac{\\sigma ^{2}}{2}\\left[\\sum_{i=1}^{N}r_{i}^{2}\\right] + N \\ln \\sigma$$\n",
    "\n",
    "which, when differentiated wrt $\\gamma$ gives us\n",
    "\n",
    "\n",
    "$$\\sigma ^{*}= \\sqrt{\\frac{\\sum_{i=1}^{N}r_{i}^{2}}{N}}$$\n",
    "\n",
    "which makes intuitive sense, i.e. if the irreducible variance is larger, we expect the residuals to be larger as the model won't be able to fit the data as closely.\n",
    "\n",
    "So now we plug this into our formula for the log test cross entropy (negative test log-likelihood)\n",
    "\n",
    "In order to calculate test-log-likelihood, it's essentially the same formula, except that we must take care that $\\sigma^{*}$ is calculated on the training data whereas the residuals are now test residuals, thus the end formula is given by\n",
    "\n",
    "$$\\frac{1}{2\\left(\\sigma^{*}\\right)^{2}}\\sum_{i=1}^{N_{test}}\\left(f(x_{i})-y_{i}\\right)^{2} + N_{test}\\cdot\\ln \\sigma^{*}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max depth =  1\n",
      "sigma_star =  0.650669364198992\n",
      "Test cross entropy =  319.18020796561814\n",
      "`Normalised` Test cross entropy =  0.06383604159312363\n",
      "Max depth =  2\n",
      "sigma_star =  0.6480970236192768\n",
      "Test cross entropy =  320.9083884969723\n",
      "`Normalised` Test cross entropy =  0.06418167769939447\n",
      "Max depth =  3\n",
      "sigma_star =  0.6450593660662713\n",
      "Test cross entropy =  324.19933096349405\n",
      "`Normalised` Test cross entropy =  0.06483986619269881\n",
      "Max depth =  4\n",
      "sigma_star =  0.6344358208477924\n",
      "Test cross entropy =  330.19273996320817\n",
      "`Normalised` Test cross entropy =  0.06603854799264164\n",
      "Max depth =  5\n",
      "sigma_star =  0.625923967707416\n",
      "Test cross entropy =  347.40858520151414\n",
      "`Normalised` Test cross entropy =  0.06948171704030283\n"
     ]
    }
   ],
   "source": [
    "def test_cross_entropy(sigma, residuals):\n",
    "\n",
    "    return len(residuals)*np.log(sigma) + (0.5/(sigma*sigma))*np.sum(np.power(residuals, 2))\n",
    "\n",
    "for MD in [1,2,3,4,5]:\n",
    "\n",
    "    reg = XGBRegressor(max_depth=MD, learning_rate=0.1, objective='reg:squarederror', n_estimators=20_000, early_stopping_rounds=5, eval_metric='rmse')\n",
    "\n",
    "    reg.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n",
    "\n",
    "    train_residuals = reg.predict(X_train) - y_train\n",
    "\n",
    "    sigma_star = np.sqrt(np.mean(np.power(train_residuals, 2)))\n",
    "\n",
    "    test_preds = reg.predict(X_test)\n",
    "    test_residuals = test_preds - y_test\n",
    "    print(\"Max depth = \", MD)\n",
    "    print(\"sigma_star = \", sigma_star)\n",
    "    print(\"Test cross entropy = \", test_cross_entropy(sigma_star, test_residuals))\n",
    "    print(\"`Normalised` Test cross entropy = \", (1/len(test_residuals))*test_cross_entropy(sigma_star, test_residuals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So the GW test-cross entropy is giving values of around 0.07 which should set the lengthscale to beat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-loss=8.11743, val-loss=8.15618\n",
      "[1]\ttrain-loss=8.03464, val-loss=8.07339\n",
      "[2]\ttrain-loss=7.95475, val-loss=7.99358\n",
      "[3]\ttrain-loss=7.87775, val-loss=7.91662\n",
      "[4]\ttrain-loss=7.80356, val-loss=7.84235\n",
      "[5]\ttrain-loss=7.73228, val-loss=7.77114\n",
      "[6]\ttrain-loss=7.66376, val-loss=7.70256\n",
      "[7]\ttrain-loss=7.59811, val-loss=7.63703\n",
      "[8]\ttrain-loss=7.53519, val-loss=7.57408\n",
      "[9]\ttrain-loss=7.47513, val-loss=7.51421\n",
      "[10]\ttrain-loss=7.41770, val-loss=7.45652\n",
      "[11]\ttrain-loss=7.36320, val-loss=7.40219\n",
      "[12]\ttrain-loss=7.31125, val-loss=7.34995\n",
      "[13]\ttrain-loss=7.26229, val-loss=7.30120\n",
      "[14]\ttrain-loss=7.21598, val-loss=7.25498\n",
      "[15]\ttrain-loss=7.17254, val-loss=7.21187\n",
      "[16]\ttrain-loss=7.13164, val-loss=7.17069\n",
      "[17]\ttrain-loss=7.09377, val-loss=7.13315\n",
      "[18]\ttrain-loss=7.05842, val-loss=7.09744\n",
      "[19]\ttrain-loss=7.02614, val-loss=7.06561\n",
      "[20]\ttrain-loss=6.99637, val-loss=7.03553\n",
      "[21]\ttrain-loss=6.96978, val-loss=7.00940\n",
      "[22]\ttrain-loss=6.94564, val-loss=6.98503\n",
      "[23]\ttrain-loss=6.92481, val-loss=6.96471\n",
      "[24]\ttrain-loss=6.90625, val-loss=6.94579\n",
      "[25]\ttrain-loss=6.89129, val-loss=6.93128\n",
      "[26]\ttrain-loss=6.87859, val-loss=6.91827\n",
      "[27]\ttrain-loss=6.86966, val-loss=6.90994\n",
      "[28]\ttrain-loss=6.86295, val-loss=6.90289\n",
      "[29]\ttrain-loss=6.86017, val-loss=6.90072\n",
      "[30]\ttrain-loss=6.85990, val-loss=6.90030\n",
      "[31]\ttrain-loss=6.86349, val-loss=6.90461\n",
      "[32]\ttrain-loss=6.86933, val-loss=6.91005\n",
      "[33]\ttrain-loss=6.88013, val-loss=6.91973\n",
      "[34]\ttrain-loss=6.89381, val-loss=6.93417\n",
      "[35]\ttrain-loss=6.90974, val-loss=6.94974\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = TinyXGBRegressor(\n",
    "    objective=\"distribution:normal\",\n",
    "    max_depth=2,\n",
    "    n_estimators=10000,\n",
    "    early_stopping_rounds=5,\n",
    "    learning_rate=0.01,\n",
    ")\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_set=(X_test, y_test),\n",
    "    verbose=10,\n",
    ")\n",
    "preds = model.predict(X_test)\n",
    "loc, scale = preds[:, 0], preds[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do these val losses look right? \n",
    "\n",
    "Here, $\\sigma(x_{i}) = e^{-\\gamma (x_{i})}$\n",
    "\n",
    "Thus by my reckoning the log-likelihood is given by\n",
    "\n",
    "$$L=\\prod_{i=1}^{N}\\frac{1}{\\sqrt{2\\pi}}e^{\\gamma(x_{i})}e^{-\\frac{e^{2\\gamma(x_{i})}}{2}\\left(y_{i}-f(x_{i})\\right)^{2}}$$\n",
    "\n",
    "so after taking the log and disgarding constants, we have\n",
    "\n",
    "$$\\sum_{i=1}^{N}\\left[\\gamma(x_{i})-\\frac{1}{2}e^{2\\gamma(x_{i})}\\left(y_{i}-f(x_{i})\\right)^{2}\\right]$$\n",
    "\n",
    "and thus the cross-entropy is given by the negative of this\n",
    "\n",
    "$$\\sum_{i=1}^{N}\\left[\\frac{1}{2}e^{2\\gamma(x_{i})}\\left(y_{i}-f(x_{i})\\right)^{2}-\\gamma(x_{i})\\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1222602.9001742941\n",
      "244.52058003485882\n"
     ]
    }
   ],
   "source": [
    "def test_cross_entropy(regressor, x, y):\n",
    "    preds = model.predict(x)\n",
    "    y_preds, gamma_preds = preds[:, 0], preds[:, 1]\n",
    "    \n",
    "    residuals = np.power(y_preds - y, 2)\n",
    "    \n",
    "    return 0.5*np.dot(np.exp(2*gamma_preds), residuals) - np.sum(gamma_preds)\n",
    "\n",
    "print(test_cross_entropy(model, X_test, y_test))\n",
    "print(test_cross_entropy(model, X_test, y_test)/len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
